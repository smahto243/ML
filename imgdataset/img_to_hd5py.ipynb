{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_imgdata_label(hdf5_path, image_path, label_pattern, shuffle_data=True):\n",
    "    '''\n",
    "    shuffle_data = True  # shuffle the images\n",
    "\n",
    "    hdf5_path = '/home/santosh/test/ML/dog_dataset.hdf5'  # hdf5_path to save the hdf5 file\n",
    "    image_path = '/home/santosh/test/ML/images/merged_images/*.jpg'\n",
    "\n",
    "    label_pattern = 'car*'\n",
    "    '''\n",
    "    # Read the images and labels from a given folder\n",
    "    imgpath = glob.glob(image_path)\n",
    "    labels = []\n",
    "    for i in imgpath:\n",
    "        label = i.split(os.path.sep)[-1].split(\".\")[0]\n",
    "        if fnmatch.fnmatch(label, label_pattern):\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "        \n",
    "    # Shuffle the data\n",
    "    if shuffle_data:\n",
    "        c = list(zip(imgpath, labels))\n",
    "        shuffle(c)\n",
    "        imgpath, labels = zip(*c)\n",
    "    return imgpath, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slice_data(imgpath , labels, slicep = (60, 20, 20) ):\n",
    "    \n",
    "    x1 = slicep[0]/100.\n",
    "    x2 = (slicep[0] + slicep[1])/100.\n",
    "    \n",
    "    # Divide the data into 60% train, 20% validation, and 20% test\n",
    "    train_imgdata = imgpath[0:int(x1*len(imgpath))]\n",
    "    train_labels = labels[0:int(x1*len(labels))]\n",
    "\n",
    "    val_imgdata = imgpath[int(x1*len(imgpath)):int(x2*len(imgpath))]\n",
    "    val_labels = labels[int(x1*len(imgpath)):int(x2*len(imgpath))]\n",
    "\n",
    "    test_imgdata = imgpath[int(x2*len(imgpath)):]\n",
    "    test_labels = labels[int(x2*len(imgpath)):]\n",
    "    \n",
    "    imgvector = {}\n",
    "    \n",
    "    imgvector['train_imgdata'] = train_imgdata\n",
    "    imgvector['train_labels'] = train_labels\n",
    "    imgvector['val_imgdata'] =  val_imgdata\n",
    "    imgvector['val_labels'] = val_labels\n",
    "    imgvector['test_imgdata'] = test_imgdata\n",
    "    imgvector['test_labels'] = test_labels\n",
    "    \n",
    "    return imgvector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hdf5(imgvector, data_order='tf'):\n",
    "    \n",
    "    #'th' for Theano, 'tf' for Tensorflow\n",
    "     \n",
    "    # check the order of data and chose proper data shape to save images\n",
    "    \n",
    "    if data_order == 'th':\n",
    "        train_shape = (len(imgvector['train_imgdata']), 3, 224, 224)\n",
    "        val_shape = (len(imgvector['val_imgdata']), 3, 224, 224)\n",
    "        test_shape = (len(imgvector['test_imgdata']), 3, 224, 224)\n",
    "    elif data_order == 'tf':\n",
    "        train_shape = (len(imgvector['train_imgdata']), 224, 224, 3)\n",
    "        val_shape = (len(imgvector['val_imgdata']), 224, 224, 3)\n",
    "        test_shape = (len(imgvector['test_imgdata']), 224, 224, 3)\n",
    "\n",
    "    # open a hdf5 file and create earrays\n",
    "    hdf5_file = h5py.File(hdf5_path, mode='w')\n",
    "\n",
    "    hdf5_file.create_dataset(\"train_img\", train_shape, np.int8)\n",
    "    hdf5_file.create_dataset(\"val_img\", val_shape, np.int8)\n",
    "    hdf5_file.create_dataset(\"test_img\", test_shape, np.int8)\n",
    "\n",
    "    hdf5_file.create_dataset(\"train_mean\", train_shape[1:], np.float32)\n",
    "\n",
    "    hdf5_file.create_dataset(\"train_labels\", (len(imgvector['train_imgdata']),), np.int8)\n",
    "    hdf5_file[\"train_labels\"][...] = imgvector['train_labels']\n",
    "    hdf5_file.create_dataset(\"val_labels\", (len(imgvector['val_imgdata']),), np.int8)\n",
    "    hdf5_file[\"val_labels\"][...] = imgvector['val_labels']\n",
    "    hdf5_file.create_dataset(\"test_labels\", (len(imgvector['test_imgdata']),), np.int8)\n",
    "    hdf5_file[\"test_labels\"][...] = imgvector['test_labels']\n",
    "\n",
    "    hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import glob\n",
    "import fnmatch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    \n",
    "    shuffle_data = True  # shuffle the images\n",
    "\n",
    "    hdf5_path ='G:/ML/dog_dataset.hdf5'  # hdf5_path to save the hdf5 file\n",
    "    image_path ='G:/ML/images/merged_images/*.jpg'\n",
    "\n",
    "    label_pattern = 'car*'\n",
    "\n",
    "    \n",
    "    imgpath, labels = read_imgdata_label(hdf5_path, image_path, label_pattern, shuffle_data)\n",
    "    \n",
    "    imgvector = slice_data(imgpath , labels, slicep = (60, 20, 20) )\n",
    "    \n",
    "    create_hdf5(imgvector, data_order='tf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
